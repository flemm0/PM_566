---
title: "Assignment 04: HPC and SQL"
author: "Flemming Wu"
date: "`r Sys.Date()`"
output: github_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
rm(list=ls())
```

```{r load libraries, warning=FALSE, message=FALSE}
library(microbenchmark)
library(parallel)
library(RSQLite)
library(DBI)
```


## HPC

### Problem 1: Make sure your code is nice

Rewrite the following R functions to make them faster. It is OK (and recommended) to take a look at Stackoverflow and Google

```{r rewrite functions}
# Total row sums
fun1 <- function(mat) { # sum each row in a matrix
  n <- nrow(mat)
  ans <- double(n) 
  for (i in 1:n) {
    ans[i] <- sum(mat[i, ])
  }
  ans
}

fun1alt <- function(mat) {
  rowSums(mat)
}

# Cumulative sum by row
fun2 <- function(mat) {
  n <- nrow(mat)
  k <- ncol(mat)
  ans <- mat
  for (i in 1:n) {
    for (j in 2:k) {
      ans[i,j] <- mat[i, j] + ans[i, j - 1]
    }
  }
  ans
}

fun2alt <- function(mat) {
  t(apply(mat, 1, cumsum))
}


# Use the data with this code
set.seed(2315)
dat <- matrix(rnorm(200 * 100), nrow = 200)

# Test for the first
mb1 <- microbenchmark::microbenchmark(
  fun1(dat),
  fun1alt(dat), check = "equivalent"
)

# Test for the second
mb2 <- microbenchmark::microbenchmark(
  fun2(dat),
  fun2alt(dat), check = "equivalent"
)


summary(mb1, unit = "relative")

summary(mb2, unit = "relative")
```
The last argument, check = “equivalent”, is included to make sure that the functions return the same result.


### Problem 2: Make things run faster with parallel computing

The following function allows simulating pi

```{r simulate pi}
sim_pi <- function(n = 1000, i = NULL) {
  p <- matrix(runif(n*2), ncol = 2)
  mean(rowSums(p^2) < 1) * 4
}

# Here is an example of the run
set.seed(156)
sim_pi(1000) # 3.132
```

In order to get accurate estimates, we can run this function multiple time, with the following code:

```{r rerun simulation of pi many times}
# This runs the simulation a 4,000 times, each with 10,000 points
set.seed(1231)
system.time({
  ans <- unlist(lapply(1:4000, sim_pi, n = 10000))
  print(mean(ans))
})
```

Rewrite the previous code using `parLapply()` to make it run faster. Make sure you set the seed using `clusterSetRNGStream()`:

```{r parallelize previous code}
n.cores <- detectCores() 
cl <- makePSOCKcluster(n.cores) 

clusterSetRNGStream(cl, 1231) # set seed

clusterEvalQ(cl, {
  paste0("Hello from process #", Sys.getpid())
})

system.time({
  clusterExport(cl, "sim_pi")
  ans <- unlist(
    parLapply( 
      cl,
      1:4000, sim_pi, n = 10000
    )
  )
  print(mean(ans))
  stopCluster(cl)
})


```

## SQL

Set up a temporary database by running the following chunk

```{r set up temporary sql database}
# install.packages(c("RSQLite", "DBI"))

library(RSQLite)
library(DBI)

# Initialize a temporary in memory database
con <- dbConnect(SQLite(), ":memory:")

# Download tables
film <- read.csv("https://raw.githubusercontent.com/ivanceras/sakila/master/csv-sakila-db/film.csv")
film_category <- read.csv("https://raw.githubusercontent.com/ivanceras/sakila/master/csv-sakila-db/film_category.csv")
category <- read.csv("https://raw.githubusercontent.com/ivanceras/sakila/master/csv-sakila-db/category.csv")

# Copy data.frames to database
dbWriteTable(con, "film", film)
dbWriteTable(con, "film_category", film_category)
dbWriteTable(con, "category", category)
```

When you write a new chunk, remember to replace the r with sql, connection=con. Some of these questions will reqruire you to use an inner join. Read more about them here (https://www.w3schools.com/sql/sql_join_inner.asp)[https://www.w3schools.com/sql/sql_join_inner.asp]
 
### Question 1
How many many movies is there available in each rating category.
```{sql connection=con}
SELECT rating, COUNT(rating) num_films  
FROM film
GROUP BY rating
ORDER BY num_films DESC
```


### Question 2
What is the average replacement cost and rental rate for each rating category.

```{sql connection=con}


SELECT c.name rating_cat, AVG(replacement_cost) avg_repl_cost, AVG(rental_rate) avg_rent_rate
FROM film f
JOIN film_category fc
ON f.film_id = fc.film_id
JOIN category c
ON fc.category_id = c.category_id
GROUP BY c.name
ORDER BY avg_repl_cost DESC, avg_rent_rate DESC


```


### Question 3
Use table film_category together with film to find the how many films there are with each category ID
```{sql connection=con}
SELECT fc.category_id, COUNT(*) num_films
FROM film_category fc
JOIN film f
ON fc.film_id = f.film_id
GROUP BY fc.category_id
```

### Question 4
Incorporate table category into the answer to the previous question to find the name of the most popular category.
```{sql connection=con}
SELECT fc.category_id, c.name category, COUNT(*) AS num_films
FROM film_category fc
JOIN film f
ON fc.film_id = f.film_id
LEFT JOIN category c
ON fc.category_id = c.category_id
GROUP BY fc.category_id
ORDER BY num_films DESC


```




```{r disconnect from connection}
dbDisconnect(con)
```

