---
title: "06-Lab Text Mining"
author: "Flemming Wu"
date: "`r Sys.Date()`"
output: github_document
always_allow_html: true
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text.
- Use dplyr and ggplot2 to analyze text data

# Lab description

For this lab we will be working with a new dataset. The dataset contains transcription samples from https://www.mtsamples.com/. And is loaded and "fairly" cleaned at https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv.

This markdown document should be rendered using `github_document` document.

### Setup packages

You should load in `dplyr`, (or `data.table` if you want to work that way), `ggplot2` and `tidytext`.

```{r load libraries, message = FALSE}
library(tidyverse)
library(tidytext)
library(knitr)
library(forcats)
```

### read in Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r download data, warning = FALSE, message = FALSE}
if(!file.exists("mtsamples.csv")){
  download.file(url = "https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv",
                destfile = "mtsamples.csv",
                method = "libcurl",
                timeout = 60)
}

mtsamples <- read.csv("mtsamples.csv")

#str(mtsamples) 

#change to tibble format
mtsamples <- as.tibble(mtsamples)

str(mtsamples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different categories do we have? Are these catagories related? overlapping? evenly distributed?

```{r count categories (medical-specialties)}
specialties <- mtsamples %>%
  count(medical_specialty, sort = TRUE)

knitr::kable(specialties)
```
There are `r nrow(specialties)` medical specialties.


```{r plot medical-specialties}
specialties %>%
  top_n(10) %>%
  ggplot(aes(x = n, y = fct_reorder(medical_specialty, n))) +
  geom_col()
```
\
The distribution is not uniform among all of the categories. Even within the top 10 categories, the distrubution is very uneven. The largest category of medical specialty is surgery.


---

## Question 2

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words

Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r tokenize transcription column and visualize top 20}
mtsamples %>%
  unnest_tokens(token, transcription) %>%
  count(token) %>%
  top_n(20, n) %>%
  ggplot(aes(x = n, y = fct_reorder(token, n))) +
  geom_col()
```
\
The results do make sense, with common stop words such as "the", "and", "was", "of", etc. showing up in the majority of the top 20 tokens in the transcription column. Additionally, the high frequency of the word "patient" is expected.

---

## Question 3

- Redo visualization but remove stopwords before
- Bonus points if you remove numbers as well

What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

```{r tokenize and remove stop words}
mtsamples %>%
  unnest_tokens(token, transcription) %>%
  count(token, sort = TRUE) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  filter( !grepl(pattern = "^[0-9]+$", x = token)) %>% #regex step to remove numbers
  top_n(20, n) %>%
  ggplot(aes(x = n, y = fct_reorder(token, n))) +
  geom_col()
```
\
With the stop words removed, we are now left with words that are associated with medical terminology such as "procedure", "pain", "blood", "anesthesia", "disease", etc. It is now clear to anyone looking at the column chart that the text being visualized is medical-related.

---

# Question 4

repeat question 2, but this time tokenize into bi-grams. how does the result change if you look at tri-grams?

```{r, cache = TRUE}
mtsamples %>%
  unnest_ngrams(bigram, transcription, n = 2) %>%
  count(bigram, sort = TRUE) %>%
  top_n(20, n) %>%
  ggplot(aes(x = n, y = fct_reorder(bigram, n))) +
  geom_col()

mtsamples %>%
  unnest_ngrams(trigram, transcription, n = 3) %>%
  count(trigram, sort = TRUE) %>%
  top_n(20, n) %>%
  ggplot(aes(x = n, y = fct_reorder(trigram, n))) +
  geom_col()
```
\
Trigrams seemed to turn up a few more medical word groups than bigrams.

---

# Question 5

Using the results you got from questions 4. Pick a word and count the words that appears after and before it.

```{r count words coming after the word blood, cache = TRUE}
mtsamples %>%
  unnest_ngrams(bigram, transcription, n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  select(word1, word2) %>%
  anti_join(stop_words, by = c("word2" = "word")) %>%
  filter(word1 == "blood") %>%
  count(word2, sort = TRUE)
```
```{r count words that come before the word blood, cache = TRUE}
mtsamples %>%
  unnest_ngrams(bigram, transcription, n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  select(word1, word2) %>%
  anti_join(stop_words, by = c("word1" = "word")) %>%
  filter(word2 == "blood") %>%
  count(word1, sort = TRUE)
```

---

# Question 6 

Which words are most used in each of the specialties. you can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?

```{r top 5 words per specialty, cache = TRUE}
mtsamples %>%
  unnest_tokens(token, transcription) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  filter( !grepl(pattern = "^[0-9]+$", x = token)) %>%
  select(medical_specialty, token) %>%
  group_by(medical_specialty) %>%
  count(token, sort = TRUE) %>%
  top_n(5, n) %>%
  arrange(medical_specialty) %>%
  knitr::kable()
```




# Question 7 - extra

Find your own insight in the data:

Ideas:

- Interesting ngrams
- See if certain words are used more in some specialties then others
