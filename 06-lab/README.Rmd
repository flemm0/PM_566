---
title: "06-Lab Text Mining"
author: "Flemming Wu"
date: "`r Sys.Date()`"
output: github_document
always_allow_html: true
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Learning goals

- Use `unnest_tokens()` and `unnest_ngrams()` to extract tokens and ngrams from text.
- Use dplyr and ggplot2 to analyze text data

# Lab description

For this lab we will be working with a new dataset. The dataset contains transcription samples from https://www.mtsamples.com/. And is loaded and "fairly" cleaned at https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv.

This markdown document should be rendered using `github_document` document.

### Setup packages

You should load in `dplyr`, (or `data.table` if you want to work that way), `ggplot2` and `tidytext`.

```{r load libraries, message = FALSE}
library(tidyverse)
library(tidytext)
library(knitr)
library(forcats)
```

### read in Medical Transcriptions

Loading in reference transcription samples from https://www.mtsamples.com/

```{r download data, warning = FALSE, message = FALSE}
if(!file.exists("mtsamples.csv")){
  download.file(url = "https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv",
                destfile = "mtsamples.csv",
                method = "libcurl",
                timeout = 60)
}

mtsamples <- read.csv("mtsamples.csv")

#str(mtsamples) 

#change to tibble format
mtsamples <- as.tibble(mtsamples)

str(mtsamples)
```

---

## Question 1: What specialties do we have?

We can use `count()` from `dplyr` to figure out how many different categories do we have? Are these catagories related? overlapping? evenly distributed?

```{r count categories (medical-specialties)}
specialties <- mtsamples %>%
  count(medical_specialty, sort = TRUE)

knitr::kable(specialties)
```
There are `r nrow(specialties)` medical specialties.


```{r plot medical-specialties}
specialties %>%
  top_n(10) %>%
  ggplot(aes(x = n, y = fct_reorder(medical_specialty, n))) +
  geom_col()
```
\
The distribution is not uniform among all of the categories. Even within the top 10 categories, the distrubution is very uneven. The largest category of medical specialty is surgery.


---

## Question 2

- Tokenize the the words in the `transcription` column
- Count the number of times each token appears
- Visualize the top 20 most frequent words

Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

```{r}
mtsamples %>%
  unnest_tokens(token, transcription)
```


---

## Question 3

- Redo visualization but remove stopwords before
- Bonus points if you remove numbers as well

What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

---

# Question 4

repeat question 2, but this time tokenize into bi-grams. how does the result change if you look at tri-grams?

---

# Question 5

Using the results you got from questions 4. Pick a word and count the words that appears after and before it.

---

# Question 6 

Which words are most used in each of the specialties. you can use `group_by()` and `top_n()` from `dplyr` to have the calculations be done within each specialty. Remember to remove stopwords. How about the most 5 used words?

# Question 7 - extra

Find your own insight in the data:

Ideas:

- Interesting ngrams
- See if certain words are used more in some specialties then others
